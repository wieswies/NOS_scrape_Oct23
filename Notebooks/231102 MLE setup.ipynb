{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdf9bbb2-3ace-4255-80e1-e0ddcbf6f99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.metrics import Accuracy\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56e2342b-2016-43f5-875e-066658829258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Number of classes: 28\n"
     ]
    }
   ],
   "source": [
    "coco = COCO('/Users/wiesruyters/Documents/WhD/Repositories/NOS_scrape/Datasets/Annotated_img_politicians/result.json')\n",
    "categories = coco.loadCats(coco.getCatIds())\n",
    "category_ids = coco.getCatIds()\n",
    "category_names = [category['name'] for category in categories]\n",
    "print(f'Number of classes: {len(category_ids)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9fe04d5a-a250-4718-980d-1ede12f4f93c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_classes = len(set(category_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9172e134-eff6-4bb2-804d-b7b0e3fd626a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of resized_images array: 81\n"
     ]
    }
   ],
   "source": [
    "image_folder = '/Users/wiesruyters/Documents/WhD/Repositories/NOS_scrape/Datasets/Annotated_img_politicians/images'\n",
    "image_height, image_width = 256, 256\n",
    "\n",
    "image_paths = [os.path.join(image_folder, filename) for filename in os.listdir(image_folder)]\n",
    "resized_images = []\n",
    "\n",
    "for image_path in image_paths:\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.resize(img, (image_width, image_height))\n",
    "    resized_images.append(img)\n",
    "\n",
    "resized_images = np.array(resized_images)\n",
    "print(f'Length of resized_images array: {len(resized_images)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af1367d8-0124-41af-9317-c20b19b19bac",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (81,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m         annotations[image_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mextend(anns)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Convert the dictionary of annotations into a list\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m annotations_list \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mlist\u001b[39m(annotations\u001b[38;5;241m.\u001b[39mvalues()))\n\u001b[1;32m     21\u001b[0m X_train, X_test, Y_train, Y_test \u001b[38;5;241m=\u001b[39m train_test_split(image_paths, annotations_list, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     22\u001b[0m Y_train_one_hot \u001b[38;5;241m=\u001b[39m to_categorical(Y_train, num_classes\u001b[38;5;241m=\u001b[39mnum_classes)\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (81,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "annotations = {}\n",
    "for category_id in category_ids:\n",
    "    image_ids = coco.getImgIds(catIds=category_id)\n",
    "    for image_id in image_ids:\n",
    "        image_info = coco.loadImgs(image_id)[0]\n",
    "        \n",
    "        # Retrieve annotations for the selected image\n",
    "        ann_ids = coco.getAnnIds(imgIds=image_info['id'], catIds=category_id)\n",
    "        anns = coco.loadAnns(ann_ids)\n",
    "        \n",
    "        # Create an entry for the image ID in the dictionary\n",
    "        if image_info['id'] not in annotations:\n",
    "            annotations[image_info['id']] = []\n",
    "        \n",
    "        # Append the annotations to the image's entry in the dictionary\n",
    "        annotations[image_info['id']].extend(anns)\n",
    "\n",
    "# Convert the dictionary of annotations into a list\n",
    "annotations_list = np.array(list(annotations.values()))\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(image_paths, annotations_list, test_size=0.2, random_state=42)\n",
    "Y_train_one_hot = to_categorical(Y_train, num_classes=num_classes)\n",
    "Y_test_one_hot = to_categorical(Y_test, num_classes=num_classes)\n",
    "\n",
    "for image_path in image_paths:  # Replace with a list of your image file paths\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.resize(img, (image_width, image_height))\n",
    "    resized_images = np.append(resized_images, [img], axis=0)\n",
    "\n",
    "resized_images = np.array(resized_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9cda980e-4c1c-414d-81f5-4cd249d38754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_1 (Conv2D)           (None, 254, 254, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 127, 127, 32)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 516128)            0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                33032256  \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 33033802 (126.01 MB)\n",
      "Trainable params: 33033802 (126.01 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(image_height, image_width, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "117d3f94-3b94-41d8-a433-bd859157d881",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'str'>\"}), (<class 'list'> containing values of types {'(<class \\'list\\'> containing values of types {\\'(<class \\\\\\'dict\\\\\\'> containing {\"<class \\\\\\'str\\\\\\'>\"} keys and {\"<class \\\\\\'int\\\\\\'>\", \"<class \\\\\\'float\\\\\\'>\", \\\\\\'(<class \\\\\\\\\\\\\\'list\\\\\\\\\\\\\\'> containing values of types {\"<class \\\\\\\\\\\\\\'float\\\\\\\\\\\\\\'>\"})\\\\\\', \"(<class \\\\\\'list\\\\\\'> containing values of types set())\"} values)\\'})'})",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m), loss\u001b[38;5;241m=\u001b[39mcategorical_crossentropy, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 3\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train, Y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39m(X_test, Y_test))\n",
      "File \u001b[0;32m~/miniconda3/envs/mle-phd/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/mle-phd/lib/python3.11/site-packages/keras/src/engine/data_adapter.py:1105\u001b[0m, in \u001b[0;36mselect_data_adapter\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1102\u001b[0m adapter_cls \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mcls\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m ALL_ADAPTER_CLS \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mcan_handle(x, y)]\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m adapter_cls:\n\u001b[1;32m   1104\u001b[0m     \u001b[38;5;66;03m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[39;00m\n\u001b[0;32m-> 1105\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1106\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to find data adapter that can handle input: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1107\u001b[0m             _type_name(x), _type_name(y)\n\u001b[1;32m   1108\u001b[0m         )\n\u001b[1;32m   1109\u001b[0m     )\n\u001b[1;32m   1110\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(adapter_cls) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1111\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1112\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData adapters should be mutually exclusive for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1113\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhandling inputs. Found multiple adapters \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m to handle \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1114\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(adapter_cls, _type_name(x), _type_name(y))\n\u001b[1;32m   1115\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'str'>\"}), (<class 'list'> containing values of types {'(<class \\'list\\'> containing values of types {\\'(<class \\\\\\'dict\\\\\\'> containing {\"<class \\\\\\'str\\\\\\'>\"} keys and {\"<class \\\\\\'int\\\\\\'>\", \"<class \\\\\\'float\\\\\\'>\", \\\\\\'(<class \\\\\\\\\\\\\\'list\\\\\\\\\\\\\\'> containing values of types {\"<class \\\\\\\\\\\\\\'float\\\\\\\\\\\\\\'>\"})\\\\\\', \"(<class \\\\\\'list\\\\\\'> containing values of types set())\"} values)\\'})'})"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.001), loss=categorical_crossentropy, metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train, epochs=50, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaf036b-7275-4b12-bb29-7db60f7a1afe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
